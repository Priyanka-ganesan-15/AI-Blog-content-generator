{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8raOGCg1vK030SbZkLoDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanka-ganesan-15/AI-Blog-content-generator/blob/main/p1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pQ0Xe_c1e7NT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vYjcqfmVezqe"
      },
      "outputs": [],
      "source": [
        "# Sentiment Analysis using PyTorch Transformer from Scratch\n",
        "# ---------------------------------------------------------------\n",
        "# Instructions:\n",
        "# - Fill in the code where you see \"Your code here\".\n",
        "# - Follow the instructions and comments provided to complete each part.\n",
        "# - Do not change the structure of the code unless necessary.\n",
        "# - Ensure that your code is well-documented and readable.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure reproducibility\n",
        "import random\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Download NLTK data (fix SSL issue by downloading manually)\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30otXX_XfAPJ",
        "outputId": "5b962f23-132e-4899-b0c0-8c885851d947"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------------------------\n",
        "# Part 1: Data Loading and Preprocessing\n",
        "# ------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "mKMoYM0KfLq7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the Dataset\n",
        "try:\n",
        "    # Load datasets using pandas.read_csv\n",
        "    df1 = pd.read_csv('amazon_cells_labelled.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "    df2 = pd.read_csv('imdb_labelled.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "    df3 = pd.read_csv('yelp_labelled.txt', delimiter='\\t', header=None, names=['sentence', 'label'])\n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset files not found. Please ensure the dataset files are in the current directory.\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "mId9gaUcfOr7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Combine and Shuffle the Dataset\n",
        "# Combine the datasets\n",
        "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "LyBGx36kfhir"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Data Cleaning and Preprocessing\n",
        "# Initialize stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Split text into tokens\n",
        "    tokens = text.split()\n",
        "    # Remove stop words\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing\n",
        "df['tokens'] = df['sentence'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "m-udttQ3fmyF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Build Vocabulary and Tokenize Text\n",
        "# Build vocabulary\n",
        "vocab_size = 10000  # Set vocabulary size\n",
        "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
        "most_common_tokens = Counter(all_tokens).most_common(vocab_size - 2)  # Reserve 2 indices for special tokens\n",
        "\n",
        "# Create mappings\n",
        "word_to_idx = {word: idx + 2 for idx, (word, _) in enumerate(most_common_tokens)}\n",
        "word_to_idx['<PAD>'] = 0\n",
        "word_to_idx['<UNK>'] = 1\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "def tokens_to_indices(tokens):\n",
        "    # Convert tokens to indices, use '<UNK>' for unknown words\n",
        "    indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "    return indices\n",
        "\n",
        "# Convert tokens to indices\n",
        "df['indices'] = df['tokens'].apply(tokens_to_indices)"
      ],
      "metadata": {
        "id": "q_nhiFeSfryZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Pad Sequences\n",
        "max_seq_length = 50  # Set maximum sequence length\n",
        "\n",
        "def pad_sequence(seq, max_len):\n",
        "    # Pad or truncate sequences\n",
        "    if len(seq) < max_len:\n",
        "        seq = seq + [word_to_idx['<PAD>']] * (max_len - len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_len]\n",
        "    return seq\n",
        "\n",
        "# Apply padding\n",
        "df['padded_indices'] = df['indices'].apply(lambda x: pad_sequence(x, max_seq_length))\n",
        "\n",
        "# Prepare feature and label arrays\n",
        "X = np.array(df['padded_indices'].tolist())\n",
        "y = df['label'].values"
      ],
      "metadata": {
        "id": "b3ZYpLBRfxyA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Split the Dataset Manually\n",
        "# Shuffle indices\n",
        "indices = np.arange(len(df))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Calculate split index\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(df) * split_ratio)\n",
        "\n",
        "# Split data\n",
        "train_indices = indices[:split_index]\n",
        "test_indices = indices[split_index:]\n",
        "\n",
        "X_train = X[train_indices]\n",
        "X_test = X[test_indices]\n",
        "y_train = y[train_indices]\n",
        "y_test = y[test_indices]"
      ],
      "metadata": {
        "id": "eJIMgui7f1pM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 2: Transformer Model Implementation\n",
        "# ------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "a36i9jvHf4Zs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instructions:\n",
        "# - Implement the Transformer model from scratch using PyTorch.\n",
        "# - Define classes for Positional Encoding, Multi-Head Self-Attention, Position-wise Feedforward Network, Transformer Encoder Layer, and the Transformer Classifier.\n",
        "# - Use the provided method signatures and fill in the code as per the instructions.\n",
        "\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "IfAhQe5Uf7No"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Initialize a positional encoding matrix 'pe' of shape [max_len, embedding_dim]\n",
        "        pe = torch.zeros(max_len, embedding_dim)\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(np.log(10000.0) / embedding_dim))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to the input embeddings 'x'\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "TdzHfeGdf_Ry"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Self-Attention Layer\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert embedding_dim % num_heads == 0  # Embedding dimension must be divisible by num_heads\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.depth = embedding_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.key = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.value = nn.Linear(embedding_dim, embedding_dim)\n",
        "        self.out = nn.Linear(embedding_dim, embedding_dim)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Split the last dimension into (num_heads, depth)\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        matmul_qk = torch.matmul(query, key.transpose(-2, -1))  # Scaled dot-product attention\n",
        "        dk = query.size()[-1]\n",
        "        scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += (mask * -1e9)  # Apply the mask\n",
        "\n",
        "        attention_weights = torch.nn.functional.softmax(scaled_attention_logits, dim=-1)\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        return output, attention_weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        query = self.split_heads(self.query(x), batch_size)\n",
        "        key = self.split_heads(self.key(x), batch_size)\n",
        "        value = self.split_heads(self.value(x), batch_size)\n",
        "\n",
        "        attention_output, attention_weights = self.attention(query, key, value)\n",
        "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.num_heads * self.depth)\n",
        "        output = self.out(attention_output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "nPMBgxNWgCRM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Position-wise Feedforward Network\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dYgh7T27gJ_u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, hidden_dim):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
        "        self.feedforward = PositionwiseFeedForward(embedding_dim, hidden_dim)\n",
        "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(embedding_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output = self.self_attention(x)\n",
        "        x = self.norm1(x + attn_output)  # Residual connection + Layer Normalization\n",
        "\n",
        "        ff_output = self.feedforward(x)\n",
        "        x = self.norm2(x + ff_output)  # Residual connection + Layer Normalization\n",
        "\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "CP9Hj7TJgM92"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VLEIgQTsgPxQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Transformer-based classifier\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, num_heads=8, num_layers=2, num_classes=1, hidden_dim=512, max_seq_length=50):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=word_to_idx['<PAD>'])\n",
        "        self.pos_encoder = PositionalEncoding(embedding_dim, max_len=max_seq_length)\n",
        "        self.transformer_encoder = TransformerEncoder(embedding_dim, num_heads, hidden_dim, num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src) * np.sqrt(self.embedding_dim)\n",
        "        embedded = self.pos_encoder(embedded)\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "        pooled_output = transformer_output.mean(dim=1)\n",
        "        output = self.fc(pooled_output)\n",
        "        output = self.sigmoid(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "tvIvxQ-9gSjY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "vocab_size = len(word_to_idx)\n",
        "model = TransformerClassifier(vocab_size)\n"
      ],
      "metadata": {
        "id": "w43VIpbigVTQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 3: Training Loop\n",
        "# ------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Y-VRz6t5gX0X"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.from_numpy(X_train).long()\n",
        "y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1)\n",
        "X_test_tensor = torch.from_numpy(X_test).long()\n",
        "y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "g8cKD1k8gamf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop parameters\n",
        "epochs = 10  # Adjust as needed\n",
        "batch_size = 32\n",
        "num_batches = int(np.ceil(len(X_train_tensor) / batch_size))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        optimizer.zero_grad()\n",
        "        # Get batch data\n",
        "        batch_X = X_train_tensor[i * batch_size:(i + 1) * batch_size]\n",
        "        batch_y = y_train_tensor[i * batch_size:(i + 1) * batch_size]\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    avg_loss = epoch_loss / num_batches\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CCubLRvgghZ",
        "outputId": "27fabadf-2d64-496f-a00a-245aab45a743"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6065\n",
            "Epoch [2/10], Loss: 0.4321\n",
            "Epoch [3/10], Loss: 0.3163\n",
            "Epoch [4/10], Loss: 0.2338\n",
            "Epoch [5/10], Loss: 0.2388\n",
            "Epoch [6/10], Loss: 0.1759\n",
            "Epoch [7/10], Loss: 0.1316\n",
            "Epoch [8/10], Loss: 0.1175\n",
            "Epoch [9/10], Loss: 0.1105\n",
            "Epoch [10/10], Loss: 0.0650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Part 4: Evaluation\n",
        "# ------------------------------------------------------\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor)\n",
        "    predicted = (outputs > 0.5).float()\n",
        "\n",
        "# Convert tensors to numpy arrays\n",
        "y_test_np = y_test_tensor.numpy()\n",
        "predicted_np = predicted.numpy()\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = y_pred.flatten()\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    accuracy = correct / len(y_true)\n",
        "    return accuracy\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = calculate_metrics(y_test_np, predicted_np)\n",
        "print(f'\\nEvaluation Metrics:')\n",
        "print(f'Accuracy  : {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkIQn7b2hXiE",
        "outputId": "41847f38-3361-4180-abe0-9a55b2934ca7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy  : 73.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------\n",
        "# Optional: Analyzing Misclassifications\n",
        "# ------------------------------------------------------\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_idx = np.where(y_test_np.flatten() != predicted_np.flatten())[0]\n",
        "\n",
        "# Display some misclassified examples\n",
        "print(\"\\nSome misclassified examples:\")\n",
        "for idx in misclassified_idx[:5]:\n",
        "    original_idx = test_indices[idx]\n",
        "    print(f\"Sentence: {df['sentence'].iloc[original_idx]}\")\n",
        "    print(f\"Actual Label: {int(y_test_np[idx])}, Predicted Label: {int(predicted_np[idx][0])}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCBy97bxheZq",
        "outputId": "7dc83a2d-a9d6-4694-b070-70b98c5b4bd4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Some misclassified examples:\n",
            "Sentence: As for the service, I thought it was good.\n",
            "Actual Label: 1, Predicted Label: 0\n",
            "\n",
            "Sentence: I would not recommend this place.\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "\n",
            "Sentence: Case was more or less an extra that I originally put on but later discarded because it scratched my ear.\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "\n",
            "Sentence: Not good when wearing a hat or sunglasses.\n",
            "Actual Label: 0, Predicted Label: 1\n",
            "\n",
            "Sentence: Also its slim enough to fit into my alarm clock docking station without removing the case.\n",
            "Actual Label: 1, Predicted Label: 0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-0f075978b5ab>:13: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(f\"Actual Label: {int(y_test_np[idx])}, Predicted Label: {int(predicted_np[idx][0])}\\n\")\n"
          ]
        }
      ]
    }
  ]
}